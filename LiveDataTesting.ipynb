{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Kristopher_2\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from car_racing import CarRacing\n",
    "\n",
    "import scipy.misc\n",
    "import imageio\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import msvcrt\n",
    "import win32api\n",
    "import win32con\n",
    "\n",
    "from multiprocess import Process, Queue, Manager\n",
    "import os\n",
    "\n",
    "from random import choice, random, randint\n",
    "\n",
    "env = CarRacing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code written by Tielessin on https://stackoverflow.com/questions/46506850/how-can-i-get-input-from-an-xbox-one-controller-in-python\n",
    "#Edited by Kristopher Karadimas\n",
    "\n",
    "from inputs import get_gamepad\n",
    "import math\n",
    "import threading\n",
    "\n",
    "class XboxController(object):\n",
    "    MAX_TRIG_VAL = math.pow(2, 8)\n",
    "    MAX_JOY_VAL = math.pow(2, 15)\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.LeftJoystickY = 0\n",
    "        self.LeftJoystickX = 0\n",
    "        self.RightJoystickY = 0\n",
    "        self.RightJoystickX = 0\n",
    "        self.LeftTrigger = 0\n",
    "        self.RightTrigger = 0\n",
    "        self.LeftBumper = 0\n",
    "        self.RightBumper = 0\n",
    "        self.A = 0\n",
    "        self.X = 0\n",
    "        self.Y = 0\n",
    "        self.B = 0\n",
    "        self.LeftThumb = 0\n",
    "        self.RightThumb = 0\n",
    "        self.Back = 0\n",
    "        self.Start = 0\n",
    "        self.LeftDPad = 0\n",
    "        self.RightDPad = 0\n",
    "        self.UpDPad = 0\n",
    "        self.DownDPad = 0\n",
    "\n",
    "        self._monitor_thread = threading.Thread(target=self._monitor_controller, args=())\n",
    "        self._monitor_thread.daemon = True\n",
    "        self._monitor_thread.start()\n",
    "\n",
    "\n",
    "    def read(self): # return the buttons/triggers that you care about in this methode\n",
    "        x = self.LeftJoystickX\n",
    "        rt = self.RightTrigger\n",
    "        lt = self.LeftTrigger\n",
    "        return [x, rt, lt]\n",
    "\n",
    "\n",
    "    def _monitor_controller(self):\n",
    "        while True:\n",
    "            events = get_gamepad()\n",
    "            for event in events:\n",
    "                if event.code == 'ABS_Y':\n",
    "                    self.LeftJoystickY = event.state / XboxController.MAX_JOY_VAL # normalize between -1 and 1\n",
    "                elif event.code == 'ABS_X':\n",
    "                    self.LeftJoystickX = event.state / XboxController.MAX_JOY_VAL # normalize between -1 and 1\n",
    "                elif event.code == 'ABS_RY':\n",
    "                    self.RightJoystickY = event.state / XboxController.MAX_JOY_VAL # normalize between -1 and 1\n",
    "                elif event.code == 'ABS_RX':\n",
    "                    self.RightJoystickX = event.state / XboxController.MAX_JOY_VAL # normalize between -1 and 1\n",
    "                elif event.code == 'ABS_Z':\n",
    "                    self.LeftTrigger = event.state / XboxController.MAX_TRIG_VAL # normalize between 0 and 1\n",
    "                elif event.code == 'ABS_RZ':\n",
    "                    self.RightTrigger = event.state / XboxController.MAX_TRIG_VAL # normalize between 0 and 1\n",
    "                elif event.code == 'BTN_TL':\n",
    "                    self.LeftBumper = event.state\n",
    "                elif event.code == 'BTN_TR':\n",
    "                    self.RightBumper = event.state\n",
    "                elif event.code == 'BTN_SOUTH':\n",
    "                    self.A = event.state\n",
    "                elif event.code == 'BTN_NORTH':\n",
    "                    self.X = event.state\n",
    "                elif event.code == 'BTN_WEST':\n",
    "                    self.Y = event.state\n",
    "                elif event.code == 'BTN_EAST':\n",
    "                    self.B = event.state\n",
    "                elif event.code == 'BTN_THUMBL':\n",
    "                    self.LeftThumb = event.state\n",
    "                elif event.code == 'BTN_THUMBR':\n",
    "                    self.RightThumb = event.state\n",
    "                elif event.code == 'BTN_SELECT':\n",
    "                    self.Back = event.state\n",
    "                elif event.code == 'BTN_START':\n",
    "                    self.Start = event.state\n",
    "                elif event.code == 'BTN_TRIGGER_HAPPY1':\n",
    "                    self.LeftDPad = event.state\n",
    "                elif event.code == 'BTN_TRIGGER_HAPPY2':\n",
    "                    self.RightDPad = event.state\n",
    "                elif event.code == 'BTN_TRIGGER_HAPPY3':\n",
    "                    self.UpDPad = event.state\n",
    "                elif event.code == 'BTN_TRIGGER_HAPPY4':\n",
    "                    self.DownDPad = event.state\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "joy = XboxController()\n",
    "\n",
    "def get_action():\n",
    "        action = env.action_space.sample()\n",
    "        inputs = joy.read()\n",
    "        \n",
    "        joystickinputfilter = 0 if (abs(inputs[0]) < 0.15) else inputs[0]\n",
    "        joystickinputsign = -1 if (joystickinputfilter<0) else 1\n",
    "        action[0] = joystickinputsign*abs((joystickinputfilter*0.65)**3)\n",
    "        action[1] = min(inputs[1],0.8)**6\n",
    "        action[2] = min(inputs[2],0.8)**6\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "bs = 32\n",
    "\n",
    "# Device configuration\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network Parameters\n",
    "num_epochs = 30\n",
    "batch_size = bs\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 64 #Dimensions of the input \"concatenatedTensor\"\n",
    "hidden_size = [32,16,8]\n",
    "num_classes = 3 #Three state outputs. Acceleration. Braking. Turning Angle\n",
    "\n",
    "#Define neural network for this part\n",
    "class StateDetectionNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(StateDetectionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size[1], hidden_size[2]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size[2], num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x.reshape(-1,input_size))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "model2 = StateDetectionNet(input_size, hidden_size, num_classes).to(device)\n",
    "model2.load_state_dict(torch.load('CurrentConditions.torch', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        esp = esp.to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "    \n",
    "image_channels = 3\n",
    "model = VAE(image_channels=image_channels).to(device)\n",
    "model.load_state_dict(torch.load('vae.torch', map_location=device))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01  0.29 -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "directory = f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "\n",
    "t = 0\n",
    "r = 0\n",
    "\n",
    "#while(True):\n",
    "action = get_action()\n",
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()\n",
    "PreviousImage = obs\n",
    "CurrentImage = obs\n",
    "\n",
    "for t in range(5000):\n",
    "    t+=1\n",
    "    action = get_action()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    r += reward\n",
    "    \n",
    "    eps=1\n",
    "    action_0 = action[0]\n",
    "    action_1 = action[1]\n",
    "    action_2 = action[2]\n",
    "    \n",
    "    PreviousImage = CurrentImage\n",
    "    CurrentImage = obs\n",
    "    \n",
    "    \n",
    "    im = Image.fromarray(np.uint8(PreviousImage)).convert('RGB')\n",
    "    im2 = Image.fromarray(np.uint8(CurrentImage)).convert('RGB')\n",
    "    \n",
    "    #CurrentImage = torch.from_numpy(np.flip(CurrentImage,axis=0).copy())\n",
    "    #PreviousImage = torch.from_numpy(np.flip(PreviousImage,axis=0).copy())\n",
    "    \n",
    "    preprocess=transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "        \n",
    "    im_preprocessed = preprocess(im2).to(device)\n",
    "    previousim_preprocessed = preprocess(im).to(device)\n",
    "    testCurrent = torch.unsqueeze(im_preprocessed, 0).to(device)\n",
    "    testPrevious = torch.unsqueeze(previousim_preprocessed, 0).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    curr, _, _ = model.encode(testCurrent)\n",
    "    prev, _, _ = model.encode(testPrevious)\n",
    "    concatenatedTensor = torch.cat([prev, curr], dim=1)\n",
    "    \n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    out = model2(concatenatedTensor).cpu().detach().numpy() \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    mean_col1 = -0.026595786253976014\n",
    "    mean_col2 = 0.21783560955090628\n",
    "    mean_col3 = 0.007642476004356518\n",
    "    sd_col1 = 0.10711904839787884\n",
    "    sd_col2 = 0.09744389996469334\n",
    "    sd_col3 = 0.043793150114235936\n",
    "    col1 = (out[:,0]*sd_col1)+mean_col1\n",
    "    col2 = (out[:,1]*sd_col2)+mean_col2\n",
    "    col3 = (out[:,2]*sd_col3)+mean_col3\n",
    "    np.set_printoptions(precision=2)\n",
    "    out = np.column_stack((col1,col2,col3))\n",
    "    print(out)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.01  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "directory = f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "\n",
    "t = 0\n",
    "r = 0\n",
    "\n",
    "#while(True):\n",
    "action = get_action()\n",
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()\n",
    "PreviousImage = obs\n",
    "CurrentImage = obs\n",
    "\n",
    "for t in range(1000):\n",
    "    t+=1\n",
    "\n",
    "    \n",
    "    PreviousImage = CurrentImage\n",
    "    CurrentImage = obs\n",
    "    \n",
    "    \n",
    "    im = Image.fromarray(np.uint8(PreviousImage)).convert('RGB')\n",
    "    im2 = Image.fromarray(np.uint8(CurrentImage)).convert('RGB')\n",
    "    \n",
    "    #CurrentImage = torch.from_numpy(np.flip(CurrentImage,axis=0).copy())\n",
    "    #PreviousImage = torch.from_numpy(np.flip(PreviousImage,axis=0).copy())\n",
    "    \n",
    "    preprocess=transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "        \n",
    "    im_preprocessed = preprocess(im2).to(device)\n",
    "    previousim_preprocessed = preprocess(im).to(device)\n",
    "    testCurrent = torch.unsqueeze(im_preprocessed, 0).to(device)\n",
    "    testPrevious = torch.unsqueeze(previousim_preprocessed, 0).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    curr, _, _ = model.encode(testCurrent)\n",
    "    prev, _, _ = model.encode(testPrevious)\n",
    "    concatenatedTensor = torch.cat([prev, curr], dim=1)\n",
    "    \n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    out = model2(concatenatedTensor).cpu().detach().numpy() \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    mean_col1 = -0.026595786253976014\n",
    "    mean_col2 = 0.21783560955090628\n",
    "    mean_col3 = 0.007642476004356518\n",
    "    sd_col1 = 0.10711904839787884\n",
    "    sd_col2 = 0.09744389996469334\n",
    "    sd_col3 = 0.043793150114235936\n",
    "    col1 = (out[:,0]*sd_col1)+mean_col1\n",
    "    col2 = (out[:,1]*sd_col2)+mean_col2\n",
    "    col3 = (out[:,2]*sd_col3)+mean_col3\n",
    "    np.set_printoptions(precision=2)\n",
    "    out = np.column_stack((col1,col2,col3))\n",
    "    print(out)\n",
    "    \n",
    "    action = get_action()\n",
    "    action[0]=col1\n",
    "    action[1]=col2\n",
    "    action[2]=col3\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    r += reward\n",
    "    \n",
    "    eps=1\n",
    "    action_0 = action[0]\n",
    "    action_1 = action[1]\n",
    "    action_2 = action[2]\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.    0.25  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "directory = f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "\n",
    "t = 0\n",
    "r = 0\n",
    "\n",
    "#while(True):\n",
    "action = get_action()\n",
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()\n",
    "PreviousImage = obs\n",
    "CurrentImage = obs\n",
    "\n",
    "for t in range(500):\n",
    "    t+=1\n",
    "    action = get_action()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    r += reward\n",
    "    \n",
    "    eps=1\n",
    "    action_0 = action[0]\n",
    "    action_1 = action[1]\n",
    "    action_2 = action[2]\n",
    "    \n",
    "    PreviousImage = CurrentImage\n",
    "    CurrentImage = obs\n",
    "    \n",
    "    \n",
    "    im = Image.fromarray(np.uint8(PreviousImage)).convert('RGB')\n",
    "    im2 = Image.fromarray(np.uint8(CurrentImage)).convert('RGB')\n",
    "    \n",
    "    #CurrentImage = torch.from_numpy(np.flip(CurrentImage,axis=0).copy())\n",
    "    #PreviousImage = torch.from_numpy(np.flip(PreviousImage,axis=0).copy())\n",
    "    \n",
    "    preprocess=transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "        \n",
    "    im_preprocessed = preprocess(im2).to(device)\n",
    "    previousim_preprocessed = preprocess(im).to(device)\n",
    "    testCurrent = torch.unsqueeze(im_preprocessed, 0).to(device)\n",
    "    testPrevious = torch.unsqueeze(previousim_preprocessed, 0).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    curr, _, _ = model.encode(testCurrent)\n",
    "    prev, _, _ = model.encode(testPrevious)\n",
    "    concatenatedTensor = torch.cat([prev, curr], dim=1)\n",
    "    \n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    out = model2(concatenatedTensor).cpu().detach().numpy() \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    mean_col1 = -0.026595786253976014\n",
    "    mean_col2 = 0.21783560955090628\n",
    "    mean_col3 = 0.007642476004356518\n",
    "    sd_col1 = 0.10711904839787884\n",
    "    sd_col2 = 0.09744389996469334\n",
    "    sd_col3 = 0.043793150114235936\n",
    "    col1 = (out[:,0]*sd_col1)+mean_col1\n",
    "    col2 = (out[:,1]*sd_col2)+mean_col2\n",
    "    col3 = (out[:,2]*sd_col3)+mean_col3\n",
    "    np.set_printoptions(precision=2)\n",
    "    out = np.column_stack((col1,col2,col3))\n",
    "    print(out)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
