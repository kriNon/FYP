{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "#from pushover import notify\n",
    "from sksq96Utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        esp = esp.to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = 3\n",
    "model = VAE(image_channels=image_channels).to(device)\n",
    "model.load_state_dict(torch.load('vae.torch', map_location=device))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDatasetFromFile(Dataset):\n",
    "    def __init__(self, folder_path, data_len, image_list):\n",
    "        \"\"\"\n",
    "        A dataset example where the class is embedded in the file names\n",
    "        This data example also does not use any torch transforms\n",
    "\n",
    "        Args:\n",
    "            folder_path (string): path to image folder\n",
    "        \"\"\"\n",
    "        # Get image list\n",
    "        self.image_list = image_list\n",
    "        ## Calculate len\n",
    "        self.data_len = data_len\n",
    "        self.num_samples = data_len\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        from PIL import Image\n",
    "        # Get image name from the pandas df\n",
    "        single_image_path = self.image_list[index]\n",
    "\n",
    "        \n",
    "        ImageNameDataList = single_image_path[39:-4]\n",
    "        imageGroupNumber = ImageNameDataList.split(\"_\")[0]\n",
    "        imageFrameNumber = ImageNameDataList.split(\"_\")[1]\n",
    "        prevFrameNumber = max(1,int(imageFrameNumber)-1)\n",
    "        \n",
    "        previous_image_path2 = glob.glob(f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'+str(imageGroupNumber)+'_'+str(prevFrameNumber)+'_'+'*')[0]\n",
    "            \n",
    "        # Open image\n",
    "        im_as_im = Image.open(single_image_path)\n",
    "        previousim_as_im = Image.open(previous_image_path2)\n",
    "        \n",
    "        preprocess=transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "        \n",
    "        im_preprocessed = preprocess(im_as_im)\n",
    "        previousim_preprocessed = preprocess(previousim_as_im)\n",
    "        return (im_preprocessed,previousim_preprocessed,ImageNameDataList)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "image_list = glob.glob(folder_path+'*')\n",
    "data_len = len(image_list)\n",
    "\n",
    "dataset = CustomDatasetFromFile(folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/',data_len=data_len, image_list=image_list)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAWAUlEQVR4nE16S49la3ZURKxv75OZ9biPbrttCQEW0MgSeABq8ZjwVyxZZgh/Agn+DSMGIIQnMLSEBS1LbR4C24Da7tt1763KzHP2tyIYfPuUfVSVKh1V7v091ooVEWvxd//od0XGxJGYmSnLF7CMQdAO8IvwSv9R8Cn631LTm03wS2SAP2LeQD9GCqgQwDP1Av5J8QP8h0AHZt4Hfy3+EfS36LdBOxVGAPAtSCIIgT8xRF7oR2hLdqoYBAAm1kdmGJgAByUESBBgBo0odFAEEJMECQ7h0Ta29wPX4EFi+Aa+BE/JMDUggGFQVRHQ7lfjIBoKaDKIYDQhEyxhmiAEIFCRwUgGsFEFbXIFTBBFoRUZQRIEDKQRBjAQcTQskowPx0rHcIlW9hp8pyGOj9sNN16EC+o98dTbD7dXvoAgeH+yFM3b1S8uVoI4PnKZW8/Jm2iBVZR50PCIYyGS+l0gqBQFZBCGIpKIBCIQdBKZAQZCT8g13chAzIAQkW40ZiKRkvqh+cLpq3MIW5SjgKfo/UVkKmbCZmNKDxwd4JWFjYgTzJmrxjYohXG1fagraIgwwETRRaBDQ+vNQAJQQGKaMALJARBH2lSbTBMNdbaEAFmuvcdAkdouD2MbVeVnz+cpD1HYwgfkkcfmPImDIkka4NfMD4wfQAp2CApS1vVvv/ivm1+wNrEwj1fPgwcISiUJADdw0OWMFVoAEBuAjT7D3ZBBixmZ4aSyBVC528CWxIU4hJIkYZNXzG+v/giJAvAUvGk80WNCgERkqA5PgpjIgby2X27srfbydmTe+ga9FlXaFONIbwE0QgckkN0BZEEgECMrwyMyZ/yTCAg5GakAiZtbwWYTDMB2t9O+sRSUn+2PwUfVC7gBBTypHjdffKARV4ggHYQ5nFfhRlhuKwaLmx5fn+p1XB9CtphRAxC7mmvxRhRipTVBhEwCkwLBCEZkAghQYCgniIE2nFSBASJqFERoVLbttuE7+OcHvo1aZDSgR/XWZtDCAUyyTzhA03PBBFQDVOwr5hw4BqZv7S5tqmEgBGAzjhjgfhCenek4CILQTpsdGFmAiwAYY5QoJ+lGdQKEMUgM1QDZVEZ/zPHLqVlBevTD414P2wtf4IyoQYbuNCeyHT2RUdySgyAHkczj5gnt0CWE57wqgb2OSkDDgRTe12ACgAQmaDs2Qq58BhgCGHHb4Dr1FIqJE6dER5AIvfbx4YYP5rHrgvlovJG3tFwrVkOaA0JqzsakPx1+hScMFEEL0+OKt9vT8Xi75SWHfATNEOIWTx1nGDkNCzYwVn2LDSiEA2FtkKQSj7WTjknFYBKaRHeCaOZyXG7POT40uzRUF9Vj5yHcwBGLnUZiT5IUQo8e7M6Lg1gOiw2ZYiFOeLRqdnkYc4OSYCbdBhOewRciAUMpQNKCFkIKCNAwkgGQIEDHAApAaLfC2ZjH9vBprw/NT1u5DGhjy2Jv21aDLXeqQIUIQKhKY7u8GcflBeydG8LAYx/1JGtq3tTzmDzaiBykO2l6Ha5y4s5CUNDBohmEqMAIgTBEMESKEj1jkIacnkzNKVQF+nRcPxy371/3ubMqV/dt7tyKKm7BJGh0UUHMhhTbAiIZK4OL1GX4Ua/lAi+5lBBo9kSnu5HIYHiGtgAQWAF0kg2CZ0Zj3QRCDNiKEqlBC63GApNSxp4N39/0nTfsg8ONGCKK5fWYAIkIQkKDI551m37G8XLwyJhJcPjox+yPOjKdQVsJZpgxbM/EGSwAXswHZXghKsnAMIAw4GJCPLc3PHkkPo5OV6iw4TAAFOWm64c5Pm1oOe4K9+QNew99xAYchGJjgiQQFrhtGdjYNXGACTZc3m/cHVwJhZidzUKqQUQVWWfUUABMOADFz7WLi9YtggSYBjFk0sFcmQ4mKggac+AZxy9v8+eTH+Jr6SK9pb6q8dWGLUeOBKRCiAwU46zzYGm7YTbmyO7khvnwOC5jv+HKlAzPJALcAGjWCh1SiwxzhVAAQYyRtW46XsnGEMjA93AiluMSA8cWR3Hg0PzmwDfkdWgoD8YXqK/H/jX5hnM0RaEwaDVJO86RYqun3JhISIqKfKhrmGN0pskCF3gjCNwECQFZq0NWkgZwegX++n6F7fm/QNnNQW4ZFUkDKlJIdfzxevzZc54nKT1s4+3QW/Cd9RRuYjGM2WajwkINiGLghh1NqqUoWSUKAwOOsaB+/RMEkEIoFHKychAQI5KACIEkeV/1Wn+AZDgAq20kcjtpZETwVteN01XgJlySx+Q9j7eeD0eNhKHgRBDJjtcnoYEENNFBoBVjCWgIQ1z1gGYQGqUzfT7DC3j+RNafM37uO6BA01hkLlsOS97QgaHs6tHf3m4/N78bNeFLUJ3deuL+bvelu8KFzpEhBSJJCOw2DdpOd0/YKYLmligiMYqwE7fT+rxO+AyTkJJwZgIE+lz5fQ8+pV/CMT5lPzAUV+BYUGe7jf7Y/sVt70ttmOkSvXXekG8vumxx82CzQQBtZ3BkQSoE0E7tWy7GTaIx4F3e6A1hU+AgWUsfWutcTwmGlQkEkCQ+9S7PSnBH06XTBt6kt9utD3QG3wwRE3nG7eev+NBjKtMc8pHE2+PDeNygm29mkkCKomDxYS5yXlaVTKMY5UiTGBshJG2uqBNALjEWA1ZVAOisVYvJkUuPnBRaHAu4V7YEGX0rYGveDl63bPJ2mcL3wTPKI8jcmxf60r4ED5VtoJnEbZYAosSCaYSgDPNI2n2bMCFxcQPKspecjZAwEUCyRVKn/gpghAapWgkBmKgVseY94qIEGQrK2bcqjmTGwHXXJ+wv7CMZ1iPrHW/vggdAfevn6ZvTwMI0ASICp+NViMKKOZ8PvabII+wGoLEYP4gOGllVF4GCAleigzRsI+dqSUmEiYS+46xOwBoQOkepNjxljjmTV8+PTDaUvQfvqS+HvnDez96CtMKdMmktLe8VDQzipqhCBJLFdWYG3N3hTlbcTsqi1TFCyBa4KlMAkwuOVz1jomV4rOzgup8CDQwWAaNnQDfrlXjW/HDTRw7uHsaOeju299C7wk4UCDSXwgMQTURBFbJYCkRUxVXchGa8CO8yEugUHIU5VXqyEKxqLdJpNoPAuNP/hUDG51p3pksPvNFMbp0LSjfm2fwYf2fdSuSRA8Wj5It5kUEDvbBsFU6fvD138k4qAHqJbiRYuJQGY8TVGS3MpM1FVxlFWAxtVRAWfPpXa+/xuepFSwGaHXoYSSjuyJY5+7vb+GZsnziOWrxKmzBizaqdybzN7imvaBa92EJoYtkXangI7OPah7fsIDNdjX3W6zUdYIINO7DAkOj0Qp61EVjJRJMil2H2FwwopyYzQQ489W5MzP1Qf498z3wMbiJHCxnE28q73B79MKBDkGmIRAI3igTMMIThdEKESVFDZbmCaDBCgwbdhz2YkqqXAigAdFIsrkpuw+GpBxbtw3rlugWd5TgDRjWIMfqCzOvtqhfy2vPSFvCI+lr4lXDvYzsGtPDYAc4KDA6xsnADoNFJR+UiJBjtXnhhtG032qf0W8GgttHiAG0GNiJR4D2vkHURjBgnTLLuYkRoAYcKO7hvFRa4H6a5M0/2V3j6wTtJt3n0DEMuxi5ChMxBku0GBBjGohFcMa3Qp0+QGIyIOzppeT7LD1KjF1NrxlnYctZoZPE5eEWRAZoOPfisvh7sOq7pP3udH64Pz4/MiATFdOvgTrGmnx0Pb7mDcaQSiouVEEQnBBXKK3iN5baK3EZxyIqIkhpur7ol8jz45TUAC44W21tpqzulXooYd8906AWvn661Md9d5y+v45l0MsKBPDFvkx3W8kcWRT9lHghWcGcCkFYNjWin0aa1snERXWbCkjI7SC0XhkZgdLj8FZ6UjVh27/oLwsAqCae+X3IzGPhm4M9FxT+/+uc9vxFnI94ehy9gDZHJQbSNzNQ6neJ6mGGmAIGiQJYCqYJUVKxiJQxqRHLSbQF2r6iQksjD6IJCc5mDWXXg1PbAeVIE12UHi2VkjCfiCfmA7dOGV2nGbO7J5jyabwzmdszOrFtsEKmT+YYniQzBzG447KggKWQnE55GunZxo8vZFm8Q24IOGkYqJEOYQRxEi6PcYwkgT3p3lgGS4eJC343xzaU+VH0nvnS/ejwNXsx3rl/F+OFjl+ft1j2zUmyhlxFFlEEvGk0Q3DyOGE1M3J5f89JD70OwXE/iICidhshaS3zPUWlR0b8wH1ZlOO/jruWhezkLA6h5xX6b280P5sPQVrPmfPD8IvnhGF9feuORnm1BpcL9s9gjuZQpR6Sm0VSAnvMwoG2XRoHTfWNbQKHZTZsnSUKSlVQiWfisd9cWsyi3Vp0/3RWc5ILm4Dc1/vySZ/Zz66CAXIpPyQ84vtr0pEs0a9z6UGshDxAIhgk4HhwGJiZiF0OMaPEAmCGnFztbzqM7PkExJ+OHlrCrfI4TCkuPnhz0szoT4MX7FntU9+EH+KJ5SW/hQ+nCvIG+3OaT5+hUqDSzfBicQGDYYaBU0bBL1AjYgBWViBAxZtQYgRg6QiUbJIAx0bzLgl7wbCfGUjACxcin0lyK4Gxn0nDi4W/I11EoeioIGvJ4V3o4Jv1MMb1nGxzrV5fhARKRXIGSckyE3cwscFiZzjU0UQypbdQYPG3DwtkXiP6Sm3jKADPReT0rhkLDpHhCxhL+YZhw8BdJn72DOHNLHvvtl0/bm/Fat63qOl/m0ei7EFrVAEDiTELu1sQejsmDuBVgueHDJNYqj8yj5zBRPDUhciJ8Tv8HABqwF2fg0o1n7nJB0SK3970BwMhHsJeAMJi8wfFF3R6d8g1WQ405V0/oL5HZU3IzzggKK1JryXzGsjftq7lYKtOYVjahemWpEa9kCEPb5AByMn6vzC4jFM730WBR+Uy6gwweC1LYBT2qfnXDj3y9vKI2JvM40s0ud3s1EnnqC5BsoDnjJGEm0WJxs9hoDEAMMapadtvTECsrPujVwz57wDi/XWR5nc9qxp/la73XJ+4FpsMMTGrAZT2C73F8ceh96SH2gYnT6zjrLBZqnzeR825zj0vEDGn4lvniPlIGiSMNdyWejoLVaTE+P4dSPrcDtPQ+7+Z3Vl/5PGYSurtbzSADXjoweiy+Z54aF6iq7WrW2ZVeWI0zgbBCU3dvA8swWTKtwrqgd0HUoMi4J/pSOh9hw4BXLt0DcoW8YBv3kD9VcEzdqbc+0zgQIjhICDGNwVyEHVxZopxntJjnZwdtyT5yiTziPrDh5XwLpoH5evhlLmDjwP6ws9hspxOU+Rny12xCClq1zPDZIPHdVYcVVnFEquV/3O88Wg26UdEe7eFIKtHJLpOTSFFYnC10KqxAiAKu6YVQUQFMJ4BoCSAzuz0CYHYCFkScQvFkNatfR50RUqBAermqlHwyH2NpvmUb3JXlyE5fqDfkW+GRfGBG8rnXgLutDVI82zu1Xgk3E5RFe/Ebx07SrmfkBr9cKYzL5n30tFzLfglzWnIJAZFepZmfX4cs3SWQYK3mfLR80bW6Do0RIUU8iW9H9mRARSnrrhcMa6URGZ/GN4uZlBPSHcQ6gRGKRmrURUTPZ+Q2kuwjVS103a0d8TMP6vSpDBZl83mjyL1XZqjUiZ1lx8BLZGLwAXmKLxOYIVhFCG0ut4aLtQNnVycoUmFlGaExTBdFomnG7NmWx/EybmNsg/srpsqXy7I1lw6wTn5MGGgsK2Udw8rvBWtJAgp1srhVQzvoe6Nb7+RH8kv2U/OiqnJ6xnBK4hDkKEkoxIis5d6f13LWF8NpF2nl+bhuu1DhCLLPCS8rmd1wqUQRjEyT5IxPLz3obpnLPPACaSqdiWadIgAm2qEBDP+g8wX5hticLViI1TFgoFYRqMXBJcSe4bK8gSrQijp9ksxNqiHu2+h5aV00bpdt3nCoEOe22ucIs3w5jhqDqzefNfrGNd4VYjW+0Ex6RX4IQcsiQEKU8B7j3RjvVW+rL8fk0TTQVFyevIWI2EKni1o2WeakMaiTCZAFcPq4HR1vh14/Pl8/fWLQmpjt2yGOZKMrjNGu3OSmUR6DVWu2aUGnnNmehEUFoEXW3WfUiqwkSAYem4/QuIyx+zbjjrqBkVSwJixGamAcODqwymsAIj5yzerTaao4mKPp603P8rWrN3JzHce4gYobQAFHz4Db2BnRI0CvKiMlE2R7oMncZIp0MmupBpLVYRlACZnQmLsx1OPANiylnVFTr7FQ1rZJV7SFy4HrC6+kCglyycNzH+CDb91GHdq4I3O77n7Vg99dx3F7oVsetcvjSG7wk9IdbwEz5vO4PuQyc5U2jH6eU6rqlgh7oM2REWuQTSGeYGsrpVaLf9z+j1+2l9pHtuImp3ThQCVb1BdfXujk46zvDoy+oXZGxnU/vGvu7rnV20rn4GvQaE/P5/b/Q9/0+u23t0e8uVF/un+/t35lHA89bm/T9bpNl7ediOfsg4/v8lj5ODfVzdfR8+inWal5e5MapYPTkwE5hmG3U56TX//6l01gWzNDyYoOnaNTA3t35niNQW3YwkmEm+CJQpkHleM6ddm8kKFLG7fn1KRvOFRUG8EeD2CgljWk0DyJ2XTIonp2NkrsiRijhmr2aJjSiJMZbGTAU6WBTypDiAX0nV6CLKhjYg+HcUMGEGjNsGJnG55QoIUIS/NxFVagkEIu2l+zz7zk7OB/Lu5ZBaaAJBswgEkWsINXhMGNDLA6hzqr2WlQ+yzJAcna6v2XX/3Gj//Opa///X/97K/8+q89f3f9mz/++//xP/3ex48fzAoeECOFTIwJH6QRQ4AJimZyNj55fwEVJFWVqH0sK2qRppifJZiEyEy0DGOpwLbF9L2ULYVzFzV3SpmTWQpVv/07v/2D948/fHz3r/7lv/gn/+gfPLx9+M2f/FU9CKOwNet71ift31Kv1CFCkDLYm3LhOaMEgQq2ADBXc8WgQ0yslkWDDTkL35ilXYYzOjVRXaM1DlWTB0azzArLHK5h7uZu7MGebOZm7sYYmuPtm7d/+J9/qsY/+7Xf/Df/999++8vX//GzP51Ho6tazGxbBFCam7QbjTwHGNGkkh4M0AgGztNqnhb8KR2ybgZ3g/nsNzLQ3M4GvSlUxV6zBl3L202vyl/3fpDWcwEA5raN3/p7f/e//fSPAP/kH/7j//rTnz5/+vj+y6/3Ly5//sffvH68Ob8RvGe+2vijL7afNH75cf7B4d+3/mc4EcCh7iYb7vR19RH+YsYTn+czEnzuqDK6j2bdt85TxxSRRV5yNlt51yZ3bZ6ArCHWPbPAZXS+//Lrf/rPf+c//Ot///v/5WfB31B+q/LlA3/t/eU3Xm4fPsw/mPh3GX+MOjDBGQgopnMfIr5rReOc7Faw4up8730Ny4ZeNujaf93v7fRw7lu7O6OfN3sKitLl5GZco/YplGnpnni99RqtU1BvxjyS75lbI4AMZpkhdztQWG2NEzE2bI0cnIY/w1AtVQ6YWjJxxR3uqpFLOzETI4jyGcR0B4Bz/f8fxEMp0GqjGTwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed input for debugging\n",
    "CurrentImage, PreviousImage, _ = next(iter(dataloader))\n",
    "print(CurrentImage.shape)\n",
    "curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "#concatenatedTensor = torch.cat((prev, curr), 0)\n",
    "\n",
    "save_image(model.decode(curr)[0], 'imagea.png')\n",
    "save_image(model.decode(prev)[0], 'imageb.png')\n",
    "\n",
    "#def decodeConcatenatedTensor(Tensor):\n",
    "#    a, b = concatenatedTensor.split(32, dim=0)\n",
    "#    aDecoded = model.decode(a)\n",
    "#    bDecoded = model.decode(b)\n",
    "#    save_image(aDecoded[0], 'imagea.png')\n",
    "#    save_image(aDecoded[0], 'imageb.png')\n",
    "    \n",
    "\n",
    "\n",
    "#decodeConcatenatedTensor(concatenatedTensor)\n",
    "from IPython.display import Image\n",
    "Image(filename='imagea.png')\n",
    "\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# recon_images = model.decode(b)\n",
    "# save_image(recon_images[0], 'test_image.png')\n",
    "# from IPython.display import Image\n",
    "# Image(filename='test_image.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Image(filename='imageb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Parameters\n",
    "num_epochs = 30\n",
    "batch_size = bs\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 64 #Dimensions of the input \"concatenatedTensor\"\n",
    "hidden_size = [32,16,8]\n",
    "num_classes = 3 #Three state outputs. Acceleration. Braking. Turning Angle\n",
    "\n",
    "#rint(concatenatedTensor.shape)\n",
    "\n",
    "#Define neural network for this part\n",
    "class StateDetectionNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(StateDetectionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size[1], hidden_size[2]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size[2], num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x.reshape(-1,input_size))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "model2 = StateDetectionNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "#Define loss and optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getResultsFromStateVariables(statevariables):\n",
    "    \n",
    "    expectedResults = np.asarray(statevariables)\n",
    "    expectedResults2 = np.char.split(expectedResults,\"_\")\n",
    "    expectedResults3 = np.asarray(expectedResults2)\n",
    "    expectedResults4 = np.stack(expectedResults3, axis=0)\n",
    "    expectedResults5 = expectedResults4[0:,2:5]\n",
    "    expectedResults6 = expectedResults5.astype(np.float)\n",
    "    \n",
    "    mean_col1 = -0.026595786253976014\n",
    "    mean_col2 = 0.21783560955090628\n",
    "    mean_col3 = 0.007642476004356518\n",
    "    sd_col1 = 0.10711904839787884\n",
    "    sd_col2 = 0.09744389996469334\n",
    "    sd_col3 = 0.043793150114235936\n",
    "    \n",
    "    col1 = (expectedResults6[:,0]-mean_col1)/sd_col1\n",
    "    col2 = (expectedResults6[:,1]-mean_col2)/sd_col2\n",
    "    col3 = (expectedResults6[:,2]-mean_col3)/sd_col3\n",
    "    expectedResults7 = np.column_stack((col1,col2,col3))\n",
    "    \n",
    "    expectedResults8 = torch.FloatTensor(expectedResults7).to(device)\n",
    "    \n",
    "    return expectedResults8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/30] Loss: 717.540823446\n",
      "Epoch[2/30] Loss: 704.844162546\n",
      "Epoch[3/30] Loss: 703.950517168\n",
      "Epoch[4/30] Loss: 710.039476987\n",
      "Epoch[5/30] Loss: 656.999815796\n",
      "Epoch[6/30] Loss: 671.114493118\n",
      "Epoch[7/30] Loss: 636.594268786\n",
      "Epoch[8/30] Loss: 675.677338474\n",
      "Epoch[9/30] Loss: 634.826619382\n",
      "Epoch[10/30] Loss: 611.271762358\n",
      "Epoch[11/30] Loss: 629.424793481\n",
      "Epoch[12/30] Loss: 584.136979026\n",
      "Epoch[13/30] Loss: 635.808228928\n",
      "Epoch[14/30] Loss: 623.776391332\n",
      "Epoch[15/30] Loss: 584.272662338\n",
      "Epoch[16/30] Loss: 593.317925572\n",
      "Epoch[17/30] Loss: 564.630451304\n",
      "Epoch[18/30] Loss: 536.991543265\n",
      "Epoch[19/30] Loss: 592.052588159\n",
      "Epoch[20/30] Loss: 574.228605211\n",
      "Epoch[21/30] Loss: 544.527180939\n",
      "Epoch[22/30] Loss: 556.810441455\n",
      "Epoch[23/30] Loss: 568.973391500\n",
      "Epoch[24/30] Loss: 537.930213384\n",
      "Epoch[25/30] Loss: 500.564814961\n",
      "Epoch[26/30] Loss: 477.562682557\n",
      "Epoch[27/30] Loss: 509.995444199\n",
      "Epoch[28/30] Loss: 527.308768606\n",
      "Epoch[29/30] Loss: 491.074565149\n",
      "Epoch[30/30] Loss: 516.252416562\n"
     ]
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('CurrentConditions.torch', map_location=device))\n",
    "#Training\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for idx, (CurrentImage, PreviousImage, statevariables) in enumerate(dataloader):\n",
    "        expectedResults = getResultsFromStateVariables(statevariables)       \n",
    "        \n",
    "        CurrentImage = CurrentImage.to(device)\n",
    "        PreviousImage = PreviousImage.to(device)\n",
    "        curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "        prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "        concatenatedTensor = torch.cat([prev, curr], dim=1)\n",
    "        #print(expectedResults)\n",
    "        \n",
    "        out = model2(concatenatedTensor)\n",
    "        loss = nn.MSELoss()\n",
    "        loss = loss(out, expectedResults)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data.item()\n",
    "        \n",
    "        #to_print = \"Loss: {:.9f}\".format(loss.data.item()/bs)\n",
    "        #print(idx)\n",
    "        #print(model2.state_dict())\n",
    "        #print(expectedResults8)\n",
    "        #print(statevariables)\n",
    "        #\n",
    "    to_print = \"Epoch[{}/{}] Loss: {:.9f}\".format(epoch+1,num_epochs, running_loss)#loss.data.item()/bs\n",
    "    print(to_print)\n",
    "    torch.save(model2.state_dict(), 'CurrentConditions.torch')\n",
    "    #print(epoch)\n",
    "        \n",
    "torch.save(model2.state_dict(), 'CurrentConditions.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs = 128\n",
    "model2.load_state_dict(torch.load('CurrentConditions.torch', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2648,  0.3936, -0.1765]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2313,  0.4547, -0.1745]], device='cuda:0')\n",
      "tensor([[ 0.0335, -0.0611, -0.0020]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "CurrentImage, PreviousImage, statevariables = next(iter(dataloader))\n",
    "curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "concatenatedTensor = torch.cat([prev, curr], dim=0)\n",
    "out = model2(concatenatedTensor)\n",
    "#out = out.cpu().detach().numpy() \n",
    "#mean_col1 = -0.026595786253976014\n",
    "#mean_col2 = 0.21783560955090628\n",
    "#mean_col3 = 0.007642476004356518\n",
    "#sd_col1 = 0.10711904839787884\n",
    "#sd_col2 = 0.09744389996469334\n",
    "#sd_col3 = 0.043793150114235936\n",
    "        \n",
    "#col1 = (out[:,0]-mean_col1)/sd_col1\n",
    "#col2 = (out[:,1]-mean_col2)/sd_col2\n",
    "#col3 = (out[:,2]-mean_col3)/sd_col3\n",
    "#out = np.column_stack((col1,col2,col3))\n",
    "#out = torch.FloatTensor(out).to(device)\n",
    "print(out)\n",
    "\n",
    "\n",
    "expectedResults = getResultsFromStateVariables(statevariables)\n",
    "print(expectedResults)\n",
    "\n",
    "print(out-expectedResults)\n",
    "\n",
    "#print(expectedResults.tolist()[0]*float(out.tolist()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0010,  0.2621,  0.0000],\n",
      "        [ 0.1692,  0.0000,  0.2621],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0344,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.0805,  0.0000,  0.2621],\n",
      "        [-0.2746,  0.0000,  0.2621],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0034,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.2621],\n",
      "        [-0.2746,  0.0000,  0.0123],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0143,  0.2621,  0.0000],\n",
      "        [ 0.2382,  0.2621,  0.0000],\n",
      "        [ 0.0022,  0.2621,  0.0000],\n",
      "        [ 0.0069,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [-0.0016,  0.2621,  0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "expectedResults = list(statevariables)\n",
    "expectedResults= np.asarray(statevariables)#statevariables[1].split(\"_\")\n",
    "expectedResults = np.char.split(expectedResults,\"_\")\n",
    "expectedResults= np.asarray(expectedResults)\n",
    "expectedResults=np.stack(expectedResults, axis=0)\n",
    "expectedResults = expectedResults[0:,2:5]\n",
    "expectedResults = expectedResults.astype(np.float)\n",
    "expectedResults = torch.from_numpy(expectedResults)\n",
    "#expectedResults = list(np.float_(expectedResults))[2:5]\n",
    "#expectedResults = torch.FloatTensor(expectedResults).to(device)\n",
    "print(expectedResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043793150114235936\n"
     ]
    }
   ],
   "source": [
    "folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "image_list = glob.glob(folder_path+'*')\n",
    "data_len = len(image_list)\n",
    "num_samples = data_len\n",
    "\n",
    "finalList = list()\n",
    "for index in range(num_samples):\n",
    "        single_image_path = image_list[index]\n",
    "        ImageNameDataList = single_image_path[39:-4]\n",
    "        finalList.append(ImageNameDataList)\n",
    "        \n",
    "\n",
    "\n",
    "expectedResults= np.asarray(finalList)#statevariables[1].split(\"_\")\n",
    "expectedResults = np.char.split(expectedResults,\"_\")\n",
    "expectedResults= np.asarray(expectedResults)\n",
    "expectedResults=np.stack(expectedResults, axis=0)\n",
    "expectedResults = expectedResults[0:,2:5]\n",
    "expectedResults = expectedResults.astype(np.float)\n",
    "\n",
    "col1 = expectedResults[:,0]\n",
    "col2 = expectedResults[:,1]\n",
    "col3 = expectedResults[:,2]\n",
    "print(np.std(col3))\n",
    "\n",
    "mean_col1 = -0.026595786253976014\n",
    "mean_col2 = 0.21783560955090628\n",
    "mean_col3 = 0.007642476004356518\n",
    "sd_col1 = 0.10711904839787884\n",
    "sd_col2 = 0.09744389996469334\n",
    "sd_col3 = 0.043793150114235936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
