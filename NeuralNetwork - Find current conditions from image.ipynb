{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "#from pushover import notify\n",
    "from sksq96Utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        esp = esp.to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = 3\n",
    "model = VAE(image_channels=image_channels).to(device)\n",
    "model.load_state_dict(torch.load('vae.torch', map_location=device))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDatasetFromFile(Dataset):\n",
    "    def __init__(self, folder_path, data_len, image_list):\n",
    "        \"\"\"\n",
    "        A dataset example where the class is embedded in the file names\n",
    "        This data example also does not use any torch transforms\n",
    "\n",
    "        Args:\n",
    "            folder_path (string): path to image folder\n",
    "        \"\"\"\n",
    "        # Get image list\n",
    "        self.image_list = image_list\n",
    "        ## Calculate len\n",
    "        self.data_len = data_len\n",
    "        self.num_samples = data_len\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        from PIL import Image\n",
    "        # Get image name from the pandas df\n",
    "        single_image_path = self.image_list[index]\n",
    "\n",
    "        \n",
    "        ImageNameDataList = single_image_path[39:-4]\n",
    "        imageGroupNumber = ImageNameDataList.split(\"_\")[0]\n",
    "        imageFrameNumber = ImageNameDataList.split(\"_\")[1]\n",
    "        prevFrameNumber = max(1,int(imageFrameNumber)-1)\n",
    "        \n",
    "        previous_image_path2 = glob.glob(f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'+str(imageGroupNumber)+'_'+str(prevFrameNumber)+'_'+'*')[0]\n",
    "            \n",
    "        # Open image\n",
    "        im_as_im = Image.open(single_image_path)\n",
    "        previousim_as_im = Image.open(previous_image_path2)\n",
    "        \n",
    "        preprocess=transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "        \n",
    "        im_preprocessed = preprocess(im_as_im)\n",
    "        previousim_preprocessed = preprocess(previousim_as_im)\n",
    "        return (im_preprocessed,previousim_preprocessed,ImageNameDataList)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "image_list = glob.glob(folder_path+'*')\n",
    "data_len = len(image_list)\n",
    "\n",
    "dataset = CustomDatasetFromFile(folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/',data_len=data_len, image_list=image_list)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed input for debugging\n",
    "CurrentImage, PreviousImage, _ = next(iter(dataloader))\n",
    "curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "#concatenatedTensor = torch.cat((prev, curr), 0)\n",
    "\n",
    "save_image(model.decode(curr)[0], 'imagea.png')\n",
    "save_image(model.decode(prev)[0], 'imageb.png')\n",
    "\n",
    "#def decodeConcatenatedTensor(Tensor):\n",
    "#    a, b = concatenatedTensor.split(32, dim=0)\n",
    "#    aDecoded = model.decode(a)\n",
    "#    bDecoded = model.decode(b)\n",
    "#    save_image(aDecoded[0], 'imagea.png')\n",
    "#    save_image(aDecoded[0], 'imageb.png')\n",
    "    \n",
    "\n",
    "\n",
    "#decodeConcatenatedTensor(concatenatedTensor)\n",
    "from IPython.display import Image\n",
    "Image(filename='imagea.png')\n",
    "\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# recon_images = model.decode(b)\n",
    "# save_image(recon_images[0], 'test_image.png')\n",
    "# from IPython.display import Image\n",
    "# Image(filename='test_image.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Image(filename='imageb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Parameters\n",
    "num_epochs = 30\n",
    "batch_size = bs\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 64 #Dimensions of the input \"concatenatedTensor\"\n",
    "hidden_size = [32,16,8]\n",
    "num_classes = 3 #Three state outputs. Acceleration. Braking. Turning Angle\n",
    "\n",
    "#rint(concatenatedTensor.shape)\n",
    "\n",
    "#Define neural network for this part\n",
    "class StateDetectionNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(StateDetectionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size[1], hidden_size[2]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size[2], num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x.reshape(-1,input_size))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "model2 = StateDetectionNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "#Define loss and optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getResultsFromStateVariables(statevariables):\n",
    "    \n",
    "    expectedResults = np.asarray(statevariables)\n",
    "    expectedResults2 = np.char.split(expectedResults,\"_\")\n",
    "    expectedResults3 = np.asarray(expectedResults2)\n",
    "    expectedResults4 = np.stack(expectedResults3, axis=0)\n",
    "    expectedResults5 = expectedResults4[0:,2:5]\n",
    "    expectedResults6 = expectedResults5.astype(np.float)\n",
    "    \n",
    "    mean_col1 = -0.026595786253976014\n",
    "    mean_col2 = 0.21783560955090628\n",
    "    mean_col3 = 0.007642476004356518\n",
    "    sd_col1 = 0.10711904839787884\n",
    "    sd_col2 = 0.09744389996469334\n",
    "    sd_col3 = 0.043793150114235936\n",
    "    \n",
    "    col1 = (expectedResults6[:,0]-mean_col1)/sd_col1\n",
    "    col2 = (expectedResults6[:,1]-mean_col2)/sd_col2\n",
    "    col3 = (expectedResults6[:,2]-mean_col3)/sd_col3\n",
    "    expectedResults7 = np.column_stack((col1,col2,col3))\n",
    "    \n",
    "    expectedResults8 = torch.FloatTensor(expectedResults7).to(device)\n",
    "    \n",
    "    return expectedResults8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/30] Loss: 3407.308091572\n",
      "Epoch[2/30] Loss: 2735.113596179\n",
      "Epoch[3/30] Loss: 2440.330503569\n",
      "Epoch[4/30] Loss: 2187.605069076\n",
      "Epoch[5/30] Loss: 1972.342315946\n",
      "Epoch[6/30] Loss: 1858.764530479\n",
      "Epoch[7/30] Loss: 1734.947466152\n",
      "Epoch[8/30] Loss: 1640.118840025\n",
      "Epoch[9/30] Loss: 1579.612260377\n",
      "Epoch[10/30] Loss: 1585.505556722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bc5452a7dee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mCurrentImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreviousImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatevariables\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mexpectedResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetResultsFromStateVariables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatevariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Kristopher_2\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Kristopher_2\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Kristopher_2\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Kristopher_2\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4a91e86cb59f>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Open image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mim_as_im\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_image_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mpreviousim_as_im\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevious_image_path2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Kristopher_2\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2877\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2878\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2879\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for idx, (CurrentImage, PreviousImage, statevariables) in enumerate(dataloader):\n",
    "        expectedResults = getResultsFromStateVariables(statevariables)       \n",
    "        \n",
    "        CurrentImage = CurrentImage.to(device)\n",
    "        PreviousImage = PreviousImage.to(device)\n",
    "        curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "        prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "        concatenatedTensor = torch.cat([prev, curr], dim=1)\n",
    "        #print(expectedResults)\n",
    "        \n",
    "        out = model2(concatenatedTensor)\n",
    "        loss = nn.MSELoss()\n",
    "        loss = loss(out, expectedResults)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data.item()\n",
    "        \n",
    "        #to_print = \"Loss: {:.9f}\".format(loss.data.item()/bs)\n",
    "        #print(idx)\n",
    "        #print(model2.state_dict())\n",
    "        #print(expectedResults8)\n",
    "        #print(statevariables)\n",
    "        #\n",
    "    to_print = \"Epoch[{}/{}] Loss: {:.9f}\".format(epoch+1,num_epochs, running_loss)#loss.data.item()/bs\n",
    "    print(to_print)\n",
    "    torch.save(model2.state_dict(), 'CurrentConditions.torch')\n",
    "    #print(epoch)\n",
    "        \n",
    "torch.save(model2.state_dict(), 'CurrentConditions.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs = 128\n",
    "model2.load_state_dict(torch.load('CurrentConditions.torch', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2022, -1.9257,  3.6844]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.7897, -2.2355,  5.8114]], device='cuda:0')\n",
      "tensor([[-0.4125,  0.3098, -2.1270]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "CurrentImage, PreviousImage, statevariables = next(iter(dataloader))\n",
    "curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "concatenatedTensor = torch.cat([prev, curr], dim=0)\n",
    "out = model2(concatenatedTensor)\n",
    "#out = out.cpu().detach().numpy() \n",
    "#mean_col1 = -0.026595786253976014\n",
    "#mean_col2 = 0.21783560955090628\n",
    "#mean_col3 = 0.007642476004356518\n",
    "#sd_col1 = 0.10711904839787884\n",
    "#sd_col2 = 0.09744389996469334\n",
    "#sd_col3 = 0.043793150114235936\n",
    "        \n",
    "#col1 = (out[:,0]-mean_col1)/sd_col1\n",
    "#col2 = (out[:,1]-mean_col2)/sd_col2\n",
    "#col3 = (out[:,2]-mean_col3)/sd_col3\n",
    "#out = np.column_stack((col1,col2,col3))\n",
    "#out = torch.FloatTensor(out).to(device)\n",
    "print(out)\n",
    "\n",
    "\n",
    "expectedResults = getResultsFromStateVariables(statevariables)\n",
    "print(expectedResults)\n",
    "\n",
    "print(out-expectedResults)\n",
    "\n",
    "#print(expectedResults.tolist()[0]*float(out.tolist()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0010,  0.2621,  0.0000],\n",
      "        [ 0.1692,  0.0000,  0.2621],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0344,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.0805,  0.0000,  0.2621],\n",
      "        [-0.2746,  0.0000,  0.2621],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0034,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.2621],\n",
      "        [-0.2746,  0.0000,  0.0123],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0143,  0.2621,  0.0000],\n",
      "        [ 0.2382,  0.2621,  0.0000],\n",
      "        [ 0.0022,  0.2621,  0.0000],\n",
      "        [ 0.0069,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [-0.0016,  0.2621,  0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "expectedResults = list(statevariables)\n",
    "expectedResults= np.asarray(statevariables)#statevariables[1].split(\"_\")\n",
    "expectedResults = np.char.split(expectedResults,\"_\")\n",
    "expectedResults= np.asarray(expectedResults)\n",
    "expectedResults=np.stack(expectedResults, axis=0)\n",
    "expectedResults = expectedResults[0:,2:5]\n",
    "expectedResults = expectedResults.astype(np.float)\n",
    "expectedResults = torch.from_numpy(expectedResults)\n",
    "#expectedResults = list(np.float_(expectedResults))[2:5]\n",
    "#expectedResults = torch.FloatTensor(expectedResults).to(device)\n",
    "print(expectedResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043793150114235936\n"
     ]
    }
   ],
   "source": [
    "folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "image_list = glob.glob(folder_path+'*')\n",
    "data_len = len(image_list)\n",
    "num_samples = data_len\n",
    "\n",
    "finalList = list()\n",
    "for index in range(num_samples):\n",
    "        single_image_path = image_list[index]\n",
    "        ImageNameDataList = single_image_path[39:-4]\n",
    "        finalList.append(ImageNameDataList)\n",
    "        \n",
    "\n",
    "\n",
    "expectedResults= np.asarray(finalList)#statevariables[1].split(\"_\")\n",
    "expectedResults = np.char.split(expectedResults,\"_\")\n",
    "expectedResults= np.asarray(expectedResults)\n",
    "expectedResults=np.stack(expectedResults, axis=0)\n",
    "expectedResults = expectedResults[0:,2:5]\n",
    "expectedResults = expectedResults.astype(np.float)\n",
    "\n",
    "col1 = expectedResults[:,0]\n",
    "col2 = expectedResults[:,1]\n",
    "col3 = expectedResults[:,2]\n",
    "print(np.std(col3))\n",
    "\n",
    "mean_col1 = -0.026595786253976014\n",
    "mean_col2 = 0.21783560955090628\n",
    "mean_col3 = 0.007642476004356518\n",
    "sd_col1 = 0.10711904839787884\n",
    "sd_col2 = 0.09744389996469334\n",
    "sd_col3 = 0.043793150114235936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
