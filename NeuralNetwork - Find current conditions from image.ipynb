{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "#from pushover import notify\n",
    "from sksq96Utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        esp = esp.to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = 3\n",
    "model = VAE(image_channels=image_channels).to(device)\n",
    "model.load_state_dict(torch.load('vae.torch', map_location=device))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDatasetFromFile(Dataset):\n",
    "    def __init__(self, folder_path, data_len, image_list):\n",
    "        \"\"\"\n",
    "        A dataset example where the class is embedded in the file names\n",
    "        This data example also does not use any torch transforms\n",
    "\n",
    "        Args:\n",
    "            folder_path (string): path to image folder\n",
    "        \"\"\"\n",
    "        # Get image list\n",
    "        self.image_list = image_list\n",
    "        ## Calculate len\n",
    "        self.data_len = data_len\n",
    "        self.num_samples = data_len\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        from PIL import Image\n",
    "        # Get image name from the pandas df\n",
    "        single_image_path = self.image_list[index]\n",
    "\n",
    "        \n",
    "        ImageNameDataList = single_image_path[39:-4]\n",
    "        imageGroupNumber = ImageNameDataList.split(\"_\")[0]\n",
    "        imageFrameNumber = ImageNameDataList.split(\"_\")[1]\n",
    "        prevFrameNumber = max(1,int(imageFrameNumber)-1)\n",
    "        \n",
    "        previous_image_path2 = glob.glob(f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'+str(imageGroupNumber)+'_'+str(prevFrameNumber)+'_'+'*')[0]\n",
    "            \n",
    "        # Open image\n",
    "        im_as_im = Image.open(single_image_path)\n",
    "        previousim_as_im = Image.open(previous_image_path2)\n",
    "        \n",
    "        preprocess=transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "        \n",
    "        im_preprocessed = preprocess(im_as_im)\n",
    "        previousim_preprocessed = preprocess(previousim_as_im)\n",
    "        return (im_preprocessed,previousim_preprocessed,ImageNameDataList)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "image_list = glob.glob(folder_path+'*')\n",
    "data_len = len(image_list)\n",
    "\n",
    "dataset = CustomDatasetFromFile(folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/',data_len=data_len, image_list=image_list)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed input for debugging\n",
    "CurrentImage, PreviousImage, _ = next(iter(dataloader))\n",
    "print(CurrentImage.shape)\n",
    "curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "#concatenatedTensor = torch.cat((prev, curr), 0)\n",
    "\n",
    "save_image(model.decode(curr)[0], 'imagea.png')\n",
    "save_image(model.decode(prev)[0], 'imageb.png')\n",
    "\n",
    "#def decodeConcatenatedTensor(Tensor):\n",
    "#    a, b = concatenatedTensor.split(32, dim=0)\n",
    "#    aDecoded = model.decode(a)\n",
    "#    bDecoded = model.decode(b)\n",
    "#    save_image(aDecoded[0], 'imagea.png')\n",
    "#    save_image(aDecoded[0], 'imageb.png')\n",
    "    \n",
    "\n",
    "\n",
    "#decodeConcatenatedTensor(concatenatedTensor)\n",
    "from IPython.display import Image\n",
    "Image(filename='imagea.png')\n",
    "\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# recon_images = model.decode(b)\n",
    "# save_image(recon_images[0], 'test_image.png')\n",
    "# from IPython.display import Image\n",
    "# Image(filename='test_image.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Image(filename='imageb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Parameters\n",
    "num_epochs = 30\n",
    "batch_size = bs\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 64 #Dimensions of the input \"concatenatedTensor\"\n",
    "hidden_size = [32,16,8]\n",
    "num_classes = 3 #Three state outputs. Acceleration. Braking. Turning Angle\n",
    "\n",
    "#rint(concatenatedTensor.shape)\n",
    "\n",
    "#Define neural network for this part\n",
    "class StateDetectionNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(StateDetectionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size[1], hidden_size[2]) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size[2], num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x.reshape(-1,input_size))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "model2 = StateDetectionNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "#Define loss and optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getResultsFromStateVariables(statevariables):\n",
    "    \n",
    "    expectedResults = np.asarray(statevariables)\n",
    "    expectedResults2 = np.char.split(expectedResults,\"_\")\n",
    "    expectedResults3 = np.asarray(expectedResults2)\n",
    "    expectedResults4 = np.stack(expectedResults3, axis=0)\n",
    "    expectedResults5 = expectedResults4[0:,2:5]\n",
    "    expectedResults6 = expectedResults5.astype(np.float)\n",
    "    \n",
    "    mean_col1 = -0.026595786253976014\n",
    "    mean_col2 = 0.21783560955090628\n",
    "    mean_col3 = 0.007642476004356518\n",
    "    sd_col1 = 0.10711904839787884\n",
    "    sd_col2 = 0.09744389996469334\n",
    "    sd_col3 = 0.043793150114235936\n",
    "    \n",
    "    col1 = (expectedResults6[:,0]-mean_col1)/sd_col1\n",
    "    col2 = (expectedResults6[:,1]-mean_col2)/sd_col2\n",
    "    col3 = (expectedResults6[:,2]-mean_col3)/sd_col3\n",
    "    expectedResults7 = np.column_stack((col1,col2,col3))\n",
    "    \n",
    "    expectedResults8 = torch.FloatTensor(expectedResults7).to(device)\n",
    "    \n",
    "    return expectedResults8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/30] Loss: 748.759565357\n",
      "Epoch[2/30] Loss: 729.585540242\n",
      "Epoch[3/30] Loss: 704.056547445\n",
      "Epoch[4/30] Loss: 703.061175407\n",
      "Epoch[5/30] Loss: 695.029658990\n",
      "Epoch[6/30] Loss: 677.158094002\n",
      "Epoch[7/30] Loss: 659.134466995\n",
      "Epoch[8/30] Loss: 657.487108619\n",
      "Epoch[9/30] Loss: 724.753099207\n",
      "Epoch[10/30] Loss: 681.450049417\n",
      "Epoch[11/30] Loss: 641.133793038\n",
      "Epoch[12/30] Loss: 619.139516890\n",
      "Epoch[13/30] Loss: 622.958074915\n",
      "Epoch[14/30] Loss: 613.505535516\n",
      "Epoch[15/30] Loss: 636.061652295\n",
      "Epoch[16/30] Loss: 620.829412074\n",
      "Epoch[17/30] Loss: 619.568808940\n",
      "Epoch[18/30] Loss: 609.630934156\n",
      "Epoch[19/30] Loss: 614.998765244\n",
      "Epoch[20/30] Loss: 592.249316667\n",
      "Epoch[21/30] Loss: 590.014225107\n",
      "Epoch[22/30] Loss: 647.765436259\n",
      "Epoch[23/30] Loss: 559.476275864\n",
      "Epoch[24/30] Loss: 580.418957630\n",
      "Epoch[25/30] Loss: 599.963191128\n",
      "Epoch[26/30] Loss: 592.826866930\n",
      "Epoch[27/30] Loss: 600.179440994\n",
      "Epoch[28/30] Loss: 570.422828909\n",
      "Epoch[29/30] Loss: 570.399484426\n",
      "Epoch[30/30] Loss: 580.956094670\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for idx, (CurrentImage, PreviousImage, statevariables) in enumerate(dataloader):\n",
    "        expectedResults = getResultsFromStateVariables(statevariables)       \n",
    "        \n",
    "        CurrentImage = CurrentImage.to(device)\n",
    "        PreviousImage = PreviousImage.to(device)\n",
    "        curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "        prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "        concatenatedTensor = torch.cat([prev, curr], dim=1)\n",
    "        #print(expectedResults)\n",
    "        \n",
    "        out = model2(concatenatedTensor)\n",
    "        loss = nn.MSELoss()\n",
    "        loss = loss(out, expectedResults)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data.item()\n",
    "        \n",
    "        #to_print = \"Loss: {:.9f}\".format(loss.data.item()/bs)\n",
    "        #print(idx)\n",
    "        #print(model2.state_dict())\n",
    "        #print(expectedResults8)\n",
    "        #print(statevariables)\n",
    "        #\n",
    "    to_print = \"Epoch[{}/{}] Loss: {:.9f}\".format(epoch+1,num_epochs, running_loss)#loss.data.item()/bs\n",
    "    print(to_print)\n",
    "    torch.save(model2.state_dict(), 'CurrentConditions.torch')\n",
    "    #print(epoch)\n",
    "        \n",
    "torch.save(model2.state_dict(), 'CurrentConditions.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs = 128\n",
    "model2.load_state_dict(torch.load('CurrentConditions.torch', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2648,  0.3936, -0.1765]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2313,  0.4547, -0.1745]], device='cuda:0')\n",
      "tensor([[ 0.0335, -0.0611, -0.0020]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "CurrentImage, PreviousImage, statevariables = next(iter(dataloader))\n",
    "curr, _, _ = model.encode(CurrentImage.to(device))\n",
    "prev, _, _ = model.encode(PreviousImage.to(device))\n",
    "concatenatedTensor = torch.cat([prev, curr], dim=0)\n",
    "out = model2(concatenatedTensor)\n",
    "#out = out.cpu().detach().numpy() \n",
    "#mean_col1 = -0.026595786253976014\n",
    "#mean_col2 = 0.21783560955090628\n",
    "#mean_col3 = 0.007642476004356518\n",
    "#sd_col1 = 0.10711904839787884\n",
    "#sd_col2 = 0.09744389996469334\n",
    "#sd_col3 = 0.043793150114235936\n",
    "        \n",
    "#col1 = (out[:,0]-mean_col1)/sd_col1\n",
    "#col2 = (out[:,1]-mean_col2)/sd_col2\n",
    "#col3 = (out[:,2]-mean_col3)/sd_col3\n",
    "#out = np.column_stack((col1,col2,col3))\n",
    "#out = torch.FloatTensor(out).to(device)\n",
    "print(out)\n",
    "\n",
    "\n",
    "expectedResults = getResultsFromStateVariables(statevariables)\n",
    "print(expectedResults)\n",
    "\n",
    "print(out-expectedResults)\n",
    "\n",
    "#print(expectedResults.tolist()[0]*float(out.tolist()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0010,  0.2621,  0.0000],\n",
      "        [ 0.1692,  0.0000,  0.2621],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0344,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.0805,  0.0000,  0.2621],\n",
      "        [-0.2746,  0.0000,  0.2621],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0034,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.2621],\n",
      "        [-0.2746,  0.0000,  0.0123],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0000,  0.2621,  0.0000],\n",
      "        [ 0.0143,  0.2621,  0.0000],\n",
      "        [ 0.2382,  0.2621,  0.0000],\n",
      "        [ 0.0022,  0.2621,  0.0000],\n",
      "        [ 0.0069,  0.2621,  0.0000],\n",
      "        [-0.2746,  0.2621,  0.0000],\n",
      "        [-0.0016,  0.2621,  0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "expectedResults = list(statevariables)\n",
    "expectedResults= np.asarray(statevariables)#statevariables[1].split(\"_\")\n",
    "expectedResults = np.char.split(expectedResults,\"_\")\n",
    "expectedResults= np.asarray(expectedResults)\n",
    "expectedResults=np.stack(expectedResults, axis=0)\n",
    "expectedResults = expectedResults[0:,2:5]\n",
    "expectedResults = expectedResults.astype(np.float)\n",
    "expectedResults = torch.from_numpy(expectedResults)\n",
    "#expectedResults = list(np.float_(expectedResults))[2:5]\n",
    "#expectedResults = torch.FloatTensor(expectedResults).to(device)\n",
    "print(expectedResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043793150114235936\n"
     ]
    }
   ],
   "source": [
    "folder_path=f'D:/Kris\\'s Workbench/FYP/TrainingData/0/'\n",
    "image_list = glob.glob(folder_path+'*')\n",
    "data_len = len(image_list)\n",
    "num_samples = data_len\n",
    "\n",
    "finalList = list()\n",
    "for index in range(num_samples):\n",
    "        single_image_path = image_list[index]\n",
    "        ImageNameDataList = single_image_path[39:-4]\n",
    "        finalList.append(ImageNameDataList)\n",
    "        \n",
    "\n",
    "\n",
    "expectedResults= np.asarray(finalList)#statevariables[1].split(\"_\")\n",
    "expectedResults = np.char.split(expectedResults,\"_\")\n",
    "expectedResults= np.asarray(expectedResults)\n",
    "expectedResults=np.stack(expectedResults, axis=0)\n",
    "expectedResults = expectedResults[0:,2:5]\n",
    "expectedResults = expectedResults.astype(np.float)\n",
    "\n",
    "col1 = expectedResults[:,0]\n",
    "col2 = expectedResults[:,1]\n",
    "col3 = expectedResults[:,2]\n",
    "print(np.std(col3))\n",
    "\n",
    "mean_col1 = -0.026595786253976014\n",
    "mean_col2 = 0.21783560955090628\n",
    "mean_col3 = 0.007642476004356518\n",
    "sd_col1 = 0.10711904839787884\n",
    "sd_col2 = 0.09744389996469334\n",
    "sd_col3 = 0.043793150114235936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
